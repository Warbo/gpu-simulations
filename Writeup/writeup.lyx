#LyX 1.6.5 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\use_default_options true
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Physics Simulations on Graphics Cards
\end_layout

\begin_layout Abstract
Abstract
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Physics, indeed the whole of science, is the attempt to describe the world
 with Mathematics.
 Computers have become an invaluable tool in this endeavour.
 Not only through the secondary, Model Theoretic, semantic interpretation
 of the side-effects caused by their programs: results to calculations,
 instant communication between collaborators around the world, mass storage
 and retrieval of knowledge on a scale much grander than the human brain
 and observations from experiments beyond the microscopic and macroscopic,
 impossible to percieve manually.
 At a much more fundamental level, the principle of operation of a computing
 device, the syntactic manipulation of symbols, is isomorphic to the application
 of Proof Theory to a deductive system
\begin_inset Note Note
status open

\begin_layout Plain Layout
Curry-Howard isomorphism
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Since the defining characteristic of Mathematics is the discovery and proof
 of theorems, by their very existence computers provide the Scientist with
 a physical manifestation of Mathematics, in fact the most powerful there
 can be
\begin_inset Note Note
status open

\begin_layout Plain Layout
Church-Turing thesis
\end_layout

\end_inset

, and thus the most useful tool there is in the endeavour to formalise the
 Universe.
 This is especially elegant for the Physicist, who's attempts to describe
 reality with Mathematical laws are thus enabled by the exploitation of
 those laws in their own derivation.
 Despite the ultimate limitations to Science formalised in this way
\begin_inset Note Note
status open

\begin_layout Plain Layout
Godel's second incompleteness theorem
\end_layout

\end_inset

, there is still far more we have yet to discover, and with the aid of computers
 we can advance our understanding at an unprecedented rate.
\end_layout

\begin_layout Standard
With the mechanisation of proof, the role of the Scientist is now solely
 that of the discoverer.
 The exploration space is theorems and the goal is surprise.
 Since a Mathematical truth is a logical consequence of its axioms and rules,
 the machine's operation makes no distinction between proving theorems and
 proving tautologies, since they are the same concept and contain no more
 information than their axioms.
 Whilst there are formalisms for beauty and interestingness built on Information
 Theory
\begin_inset Note Note
status open

\begin_layout Plain Layout
Cite compression guy
\end_layout

\end_inset

, these are not part of the inherent syntactic nature of computation, and
 can only ever exist in the secondary, Model Theory realm built atop it.
 Until we mechanise the limits of semantics, as we have done syntactically
 with the Turing machine, humans will have a definite role to play in Science:
 as the all-important heuristic which guides the search.
\end_layout

\begin_layout Subsection
Physics with Computers
\end_layout

\begin_layout Standard
Computers are only as useful as the theorems they prove, and it is the computer
 program which defines the operations to take.
 The job of the programmers is to define programs such that their algebra
 encapsulates the model under investigation.
 There are many ways to approach this task, from high-level interactive
 theorem provers to low-level machine instructions.
 Of course all are isomorphic, but are necessarily distinct in order to
 minimise the transformation from our mental model to the programming paradigm.
 In this project the simulation approach is taken, which is a low-level
 implementation that uses machine components to numerically approximate
 the objects described by the model.
\end_layout

\begin_layout Standard
The main advantage to simulations is the simple correspondance between the
 physical system of the model and the physical system of the machine.
 This makes programming a simulation a relatively straightforward affair.
 The main disadvantages of simulations are the large time and space requirements
, compared to less intensive symbol manipulation such as statistical mechanics.
 This inefficiency, caused by the massive redundancy of the system, is the
 reason why the technique only became popular after the advent of fast mechanica
l computers.
 The ability to map objects to machine components, however, keeps the overhead
 low such that available resources aren't wasted.
 From a more practical point of view, being able to perform this mapping
 manually at a low level is an advantage considering that language development
 and pattern matching are still in their infancy as Mathematical disciplines.
\end_layout

\begin_layout Standard
Another disadvantage is the error introduced by numerically approximating
 Real values in the model with Rational values in the computer.
 The magnitude of these errors depend on the resolution of the simulation
 which, for each spacetime dimension, linearly affects the amount of computation
 to perform.
 High resolution is important for Physics simulations, as many Physical
 laws are defined as differential equations, and thus simulation becomes
 discrete summation as an approximation to the continuous integration of
 the system under these laws.
\end_layout

\begin_layout Standard
The chosen resolution also depends on the physical properties of the intended
 machine, since machine instructions can only manipulate machine numbers
 and thus anything outside that range requires algorithmic abstractions.
 For example, on a 64-bit machine it is not possible to exactly represent,
 as a machine number, a binary Natural 
\begin_inset Formula $x\geq10^{1000000}+1$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Cite
\end_layout

\end_inset

, and similarly machines following the IEEE 754-2008 floating point standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Cite
\end_layout

\end_inset

 cannot store, as a machine number, a binary Rational such as 
\begin_inset Formula $10^{10000101}+\frac{1}{10}$
\end_inset

 (decimal 
\begin_inset Formula $2^{133}+\frac{1}{2}$
\end_inset

).
 To do so requires a 
\emph on
bignum
\emph default
 (arbitrary-precision arithmetic) library, which stores Integers as a linear
 datastructure (such as an array) of machine numbers, and Rationals as two
 (a numerator and a denominator).
 This severely impacts performance, as the computational complexity of every
 arithmetic operation changes from 
\begin_inset Formula $O\left(1\right)$
\end_inset

 (for machine numbers pipelined through the CPU's Arithmetic Logic Unit)
 to 
\begin_inset Formula $O\left(N_{bits}\right)$
\end_inset

 for addition and subtraction, whilst efficient multiplication algorithms
 are still a research topic (for example the 
\begin_inset Formula $O(N_{bits}\cdot\log N_{bits}\cdot2^{O(\log^{*}N_{bits})})$
\end_inset

 algorithm from
\begin_inset Note Note
status open

\begin_layout Plain Layout
Anindya De, Piyush P Kurur, Chandan Saha, Ramprasad Saptharishi.
 Fast Integer Multiplication Using Modular Arithmetic.
 Symposium on Theory of Computation (STOC) 2008.
\end_layout

\end_inset

).
\end_layout

\begin_layout Standard
Given these constraints, simulations are amongst the most computationally
 intensive practical programs in the world, which keeps up the demand for
 clustering technology and supercomputers.
 Rather than requiring such expensive technology, this project aims to implement
 Physics simulations on commonly available computer hardware by reimplementing
 algorithms to be parallel, implementing them in an efficient manner and
 distributing them to run concurrently on the graphics processors found
 in modern PCs.
\end_layout

\begin_layout Subsection
GPU Overview
\end_layout

\begin_layout Standard
A typical commodity computer (ie.
 not intended for embedded or server use) will contain a graphics card,
 either as a plug-in circuit board or integrated into the main motherboard.
 The reason for this is historical, since graphical applications like games
 can always make full use of a computer's resources no matter how extensive
 they are, eg.
 by increasing the resolution or frame-rate of the display, and are thus
 always demanding more powerful hardware.
 To keep down the cost of satiating this demand, it was sensible to offload
 graphics tasks to a co-processor which could be upgraded without having
 to buy a whole new computer.
 These co-processors are called GPUs (Graphics Processing Units) and they
 are housed along with their life-support components on the graphics card.
\end_layout

\begin_layout Standard
Real-time 3D rendering was certainly the main driving force for graphics
 card development, for example most modern GPUs no longer contain explicit
 circuitry for 2D rendering, instead using the space to expand the 3D engine
 and emulating any 2D processing calls.
 The most striking difference between 3D graphics and other common computing
 tasks is that it is an 
\emph on
embarassingly parallel
\emph default
 problem.
 In other words, it is trivial to get a near 100% increase in performance
 simply by doubling the number of computers that are working on the task
 at once.
\end_layout

\begin_layout Standard
In general such speedups are hard to achieve, since allowing many machines
 to access mutable variables at once requires 
\emph on
locks
\emph default
 to prevent conflicting changes, which themselves bring notoriously difficult
 to spot problems like 
\emph on
deadlock
\emph default
 (where two machines require two locks before accessing memory, one machine
 holds one lock, the other machine holds the other lock and neither is capable
 of proceeding).
 A trivial method, such as an optimisation pass in a compiler, for effectively
 utilising resources in parallel is a much sought-after goal in many research
 groups.
 The fact that 3D graphics scales so easily makes the difficulties of parallel
 programming seem embarassing in comparison.
\end_layout

\begin_layout Standard
The reason 3D is so inherently parallel is that there is very little dependence
 between the calculations being performed.
 This means, for example, that the screen can be split into sections, each
 sent to a different processor for rendering, then the results collected
 and displayed together.
 Any additional resources can be utilised by splitting the screen into more
 sections.
 This put GPU manufacturers in a much better position than their rival CPU
 manufacturers: instead of having to squeeze more performance out of already
 extensively optimised processors, by focusing on an embarassingly parallel
 task they could get more performance by just adding more of the existing
 processors, creating so-called 
\emph on
multiprocessors
\emph default
.
 Modern GPUs contain a number of multiprocessors, each with several 
\emph on
cores
\emph default
 (processing units), typically 8 per multiprocessor.
 In comparison, mainstream CPUs have only recently become multi-core, and
 quad-core machines are still considered high-end computers despite having
 half the processing cores of a single graphics multiprocessor, of which
 there are several in a cheap GPU.
\end_layout

\begin_layout Standard
The processing capabilities of GPUs have been considered for scientific
 computing for several years 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Cite something
\end_layout

\end_inset

 but until recently calculations would have to be translated into an analogous
 graphical task, sent to the GPU for processing, the results transferred
 back then translated into the original context.
 The overhead of conversion and transfering back and forth between the graphics
 card and the motherboard made this approach too inefficient for practical
 use, and the numerical precision of GPUs was insufficient to make large
 calculations worthwhile
\begin_inset Note Note
status open

\begin_layout Plain Layout
Cite that paper which sent calculations back and forth
\end_layout

\end_inset

.
 Recently these limitations have been overcome by the emergence of General-Purpo
se Graphics Processing Units (
\emph on
GPGPUs
\emph default
), capable of running arbitrary code rather than just graphics tasks.
 Complementary programming interfaces, like Nvidia's CUDA and the less vendor-sp
ecific OpenCL, have become available for accessing the cards from mainstream
 programming languages like C, and with this integration the device specificatio
ns have been brought up to meet the precision standards required by these
 language's compilers.
 This has spurred a stream of researchers to offload their intensive computation
s to graphics cards as a more cost-effective alternative to clusters and
 conventional supercomputers.
\end_layout

\begin_layout Section
The CUDA Model
\end_layout

\begin_layout Standard
This project focuses specifically on the use of Nvidia's CUDA (Compute Unified
 Device Architecture
\begin_inset Note Note
status open

\begin_layout Plain Layout
is that right?
\end_layout

\end_inset

) interface, so a more specific description of its operation, and its use
 from C, is given here.
\end_layout

\begin_layout Standard
CUDA is an interface to Nvidia graphics cards, a schematic of which can
 be seen in
\begin_inset Note Note
status open

\begin_layout Plain Layout
add figure
\end_layout

\end_inset

.
 The components are structured in a hierarchy focused on memory.
 The graphics card's RAM is the root of the tree, globally accessible but
 relatively slow 
\begin_inset Note Note
status open

\begin_layout Plain Layout
quote a number
\end_layout

\end_inset

.
 Below this is the shared memory of the multiprocessors, connected to global
 memory through a cache.
 This memory is accessible to every core in its multiprocessor, and this
 ultimately determines the coarse-grain structure of the algorithms used.
 Finally each core in a multiprocessor (there are 8) has its own registers,
 directly read from and written to by the Arithmetic Logic Units (the mathematic
al circuitry).
\end_layout

\begin_layout Standard
CUDA separates CPU code from GPU code, and hides the graphics card from
 the host.
 Although it is not directly accessible in the way that RAM and registers
 are accessible, CUDA does provide functions to handle the device.
 For example a call to 
\family typewriter
cudaMemcpy(void* destination, void* source, int size, cudaMemcpyHostToDevice)
\family default
 will transfer 
\family typewriter
size
\family default
 dereferenced values from the pointer 
\family typewriter
source
\family default
 to the pointer 
\family typewriter
destination
\family default
.
 In this case the flag 
\family typewriter
cudaMemcpyHostToDevice
\family default
 is passed, indicating that 
\family typewriter
source
\family default
 points to a location in the host's RAM whilst 
\family typewriter
destination
\family default
 points to a location in the GPU's RAM.
 The set-up and tear-down for a calculation is completely accomplished through
 the use of these functions, whilst the calculation is initiated using a
 new syntax.
\end_layout

\begin_layout Standard
The GPU code is written in the usual C way, but there are several semantic
 differences to be aware of.
 Firstly, there are now three distinct kinds of function: 
\emph on
host functions
\emph default
 (regular C functions running on the host), 
\emph on
device functions
\emph default
 (run on the GPU and can only be called from there) and 
\emph on
kernel functions
\emph default
 (run on the GPU but callable from the host).
 Host functions are specified in the usual way, whilst device and kernel
 functions have their signatures augmented with the keywords 
\family typewriter
__device__
\family default
 and 
\family typewriter
__global__
\family default
 respectively.
 In figure 
\begin_inset Note Note
status open

\begin_layout Plain Layout
what
\end_layout

\end_inset

 we see a simple CUDA file with prototypes for a host function 
\family typewriter
main
\family default
 (our entry point), a kernel function 
\family typewriter
foo
\family default
 and two device functions 
\family typewriter
bar
\family default
 and 
\family typewriter
baz.
 Figure 
\begin_inset Note Note
status open

\begin_layout Plain Layout

\family typewriter
what
\end_layout

\end_inset

 shows which functions can call each other.
 Note that recursion is not allowed for device or kernel functions.
\end_layout

\end_body
\end_document
