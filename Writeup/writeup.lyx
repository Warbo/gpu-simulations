#LyX 1.6.5 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{program}
\usepackage{listings}
\end_preamble
\use_default_options true
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Physics Simulations on Graphics Cards
\end_layout

\begin_layout Abstract
Abstract
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Physics, indeed the whole of science, is the attempt to describe the world
 with Mathematics.
 Computers have become an invaluable tool in this endeavour.
 Not only through the secondary, Model Theoretic, semantic interpretation
 of the side-effects caused by their programs: results to calculations,
 instant communication between collaborators around the world, mass storage
 and retrieval of knowledge on a scale much grander than the human brain
 and observations from experiments beyond the microscopic and macroscopic,
 impossible to percieve manually.
 At a much more fundamental level, the principle of operation of a computing
 device, the syntactic manipulation of symbols, is isomorphic to the application
 of Proof Theory to a deductive system
\begin_inset Note Note
status open

\begin_layout Plain Layout
Curry-Howard isomorphism
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Since the defining characteristic of Mathematics is the discovery and proof
 of theorems, by their very existence computers provide the Scientist with
 a physical manifestation of Mathematics, in fact the most powerful there
 can be
\begin_inset Note Note
status open

\begin_layout Plain Layout
Church-Turing thesis
\end_layout

\end_inset

, and thus the most useful tool there is in the endeavour to formalise the
 Universe.
 This is especially elegant for the Physicist, who's attempts to describe
 reality with Mathematical laws are thus enabled by the exploitation of
 those laws in their own derivation.
 Despite the ultimate limitations to Science formalised in this way
\begin_inset Note Note
status open

\begin_layout Plain Layout
Godel's second incompleteness theorem
\end_layout

\end_inset

, there is still far more we have yet to discover, and with the aid of computers
 we can advance our understanding at an unprecedented rate.
\end_layout

\begin_layout Standard
With the mechanisation of proof, the role of the Scientist is now solely
 that of the discoverer.
 The exploration space is theorems and the goal is surprise.
 Since a Mathematical truth is a logical consequence of its axioms and rules,
 the machine's operation makes no distinction between proving theorems and
 proving tautologies, since they are the same concept and contain no more
 information than their axioms.
 Whilst there are formalisms for beauty and interestingness built on Information
 Theory
\begin_inset Note Note
status open

\begin_layout Plain Layout
Cite compression guy
\end_layout

\end_inset

, these are not part of the inherent syntactic nature of computation, and
 can only ever exist in the secondary, Model Theory realm built atop it.
 Until we mechanise the limits of semantics, as we have done syntactically
 with the Turing machine, humans will have a definite role to play in Science:
 as the all-important heuristic which guides the search.
\end_layout

\begin_layout Subsection
Physics with Computers
\end_layout

\begin_layout Standard
Computers are only as useful as the theorems they prove, and it is the computer'
s 
\emph on
program
\emph default
 (a combination of software and hardware) which defines the operations to
 take.
 The job of the programmer is to define programs such that their algebra
 encapsulates the model under investigation.
 There are many ways to approach this task, arbitrarily categorised as 
\emph on
high-level
\emph default
 (bearing little relation to the machine, requiring pattern matching to
 implement) such 
\begin_inset Quotes eld
\end_inset

interactive theorem provers
\begin_inset Quotes erd
\end_inset

 (, virtual machines and functional to low-level machine instructions.
 Of course all are isomorphic, but are necessarily distinct in order to
 minimise the transformation from our mental model to the programming paradigm.
 In this project the simulation approach is taken, which is a 
\emph on
low-level
\emph default
 (complementary to the circuitry) implementation that uses machine components
 to numerically approximate the objects described by the model.
\end_layout

\begin_layout Standard
The main advantage to simulations is the lack of abstractions such as sets,
 ranges, derivatives, etc.
 which are often used to model the behaviour of large systems.
 In a simulation, a (usually) simple correspondance is made between the
 physical system of the model and the physical system of the machine.
 This makes programming a simulation a relatively straightforward affair,
 and the results contain much more information than the existential and
 aggregate output obtained from more abstract models.
 The proof is also stronger as it depends on the validity of fewer assumptions.
 However, this inevitably leads to the biggest problem of simulations: their
 consumption of resources.
 Less intensive symbol manipulation, such as statistical mechanics, is simple
 enough to perform manually, whereas simulation had to wait for the advent
 of fast mechanical computers before it could tackle systems of similar
 complexity.
 Despite the inefficiency of the technique, being able to act relatively
 safely at a low level keeps overhead low, such that few resources are wasted.
 From a more practical point of view, being able to perform this mapping
 manually at a low level is an advantage considering that language development
 and pattern matching are still in their infancy as Mathematical disciplines.
\end_layout

\begin_layout Standard
Another disadvantage is the error introduced by numerically approximating
 Real values of a model with the Rational values of the computer, requiring
 extra care to be taken to ensure that the rules are being applied appropriately.
 The magnitude of these errors is usually insignificant, however but the
 magnitude of these errors depend on the resolution of the simulation which,
 for each spacetime dimension, linearly affects the amount of computation
 to perform.
 High resolution is important for Physics simulations, as many Physical
 laws are defined as differential equations, and thus simulation becomes
 discrete summation as an approximation to the continuous integration of
 the system under these laws.
\end_layout

\begin_layout Standard
The chosen resolution also depends on the physical properties of the intended
 machine, since machine instructions can only manipulate machine numbers
 and thus anything outside that range requires algorithmic abstractions.
 For example, on a 64-bit machine it is not possible to exactly represent,
 as a machine number, a binary Natural 
\begin_inset Formula $x\geq10^{1000000}+1$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Cite
\end_layout

\end_inset

, and similarly machines following the IEEE 754-2008 floating point standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Cite
\end_layout

\end_inset

 cannot store, as a machine number, a binary Rational such as 
\begin_inset Formula $10^{10000101}+\frac{1}{10}$
\end_inset

 (decimal 
\begin_inset Formula $2^{133}+\frac{1}{2}$
\end_inset

).
 To do so requires a 
\emph on
bignum
\emph default
 (arbitrary-precision arithmetic) library, which stores Integers as a linear
 datastructure of machine numbers, and Rationals as two (a numerator and
 a denominator).
 This severely impacts performance, as the computational complexity of every
 arithmetic operation changes from 
\begin_inset Formula $O\left(1\right)$
\end_inset

 (for machine numbers pipelined through the CPU's Arithmetic Logic Unit)
 to 
\begin_inset Formula $O\left(N_{bits}\right)$
\end_inset

 for addition and subtraction, whilst efficient multiplication algorithms
 are still a research topic (for example the 
\begin_inset Formula $O(N_{bits}\cdot\log N_{bits}\cdot2^{O(\log^{*}N_{bits})})$
\end_inset

 algorithm from
\begin_inset Note Note
status open

\begin_layout Plain Layout
Anindya De, Piyush P Kurur, Chandan Saha, Ramprasad Saptharishi.
 Fast Integer Multiplication Using Modular Arithmetic.
 Symposium on Theory of Computation (STOC) 2008.
\end_layout

\end_inset

).
\end_layout

\begin_layout Standard
Given these constraints, Physics simulations are amongst the most computationall
y intensive programs with practical use (it is common for worthwhile programs
 to be limited by I/O rather than available cycles, and it is of course
 trivial to make intensive programs which derive no meaningful results).
 This project aims to research the computational potential of graphics rendering
 hardware to this expensive task, rather than more traditional (and costly)
 alternatives such as supercomputers and clusters.
\end_layout

\begin_layout Subsection
GPU Overview
\end_layout

\begin_layout Standard
A typical commodity computer (ie.
 not intended for embedded or server use) will contain a graphics card,
 either as a plug-in circuit board or integrated into the main motherboard.
 The reason for this is historical, since graphical applications like games
 can always make full use of a computer's resources no matter how extensive
 they are, eg.
 by increasing the resolution or frame-rate of the display, and are thus
 always demanding more powerful hardware.
 To keep down the cost of satiating this demand, it was sensible to offload
 graphics tasks to a co-processor which could be upgraded without having
 to buy a whole new computer.
 These co-processors are called GPUs (Graphics Processing Units) and they
 are housed along with their life-support components on the graphics card.
\end_layout

\begin_layout Standard
Real-time 3D rendering was certainly the main driving force for graphics
 card development, for example most modern GPUs no longer contain explicit
 circuitry for 2D rendering, instead using the space to expand the 3D engine
 and emulating any 2D processing calls.
 The most striking difference between 3D graphics and other common computing
 tasks is that it is an 
\emph on
embarrassingly parallel
\emph default
 problem.
 In other words, it is trivial to get a near 100% increase in performance
 simply by doubling the number of computers that are working on the task
 at once.
\end_layout

\begin_layout Standard
In general such speedups are hard to achieve, since allowing many machines
 to access mutable variables at once requires 
\emph on
locks
\emph default
 to prevent conflicting changes, which themselves bring notoriously difficult
 to spot problems like 
\emph on
deadlock
\emph default
 (where two machines require two locks before accessing memory, one machine
 holds one lock, the other machine holds the other lock and neither is capable
 of proceeding).
 A trivial method, such as an optimisation pass in a compiler, for effectively
 utilising resources in parallel is a much sought-after goal in many research
 groups.
 The fact that 3D graphics scales so easily makes the difficulties of parallel
 programming seem embarassing in comparison.
\end_layout

\begin_layout Standard
The reason 3D is so inherently parallel is that there is very little dependence
 between the calculations being performed.
 This means, for example, that the screen can be split into sections, each
 sent to a different processor for rendering, then the results collected
 and displayed together.
 Any additional resources can be utilised by splitting the screen into more
 sections.
 This put GPU manufacturers in a much better position than their rival CPU
 manufacturers: instead of having to squeeze more performance out of already
 extensively optimised processors, by focusing on an embarassingly parallel
 task they could get more performance by just adding more of the existing
 processors, creating so-called 
\emph on
multiprocessors
\emph default
.
 Modern GPUs contain a number of multiprocessors, each with several 
\emph on
cores
\emph default
 (processing units), typically 8 per multiprocessor.
 In comparison, mainstream CPUs have only recently become multi-core, and
 quad-core machines are still considered high-end computers despite having
 half the processing cores of a single graphics multiprocessor, of which
 there are several in a cheap GPU.
\end_layout

\begin_layout Standard
The processing capabilities of GPUs have been considered for scientific
 computing for several years 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Cite something
\end_layout

\end_inset

 but until recently calculations would have to be translated into an analogous
 graphical task, sent to the GPU for processing, the results transferred
 back then translated into the original context.
 The overhead of conversion and transfering back and forth between the graphics
 card and the motherboard made this approach too inefficient for practical
 use, and the numerical precision of GPUs was insufficient to make large
 calculations worthwhile
\begin_inset Note Note
status open

\begin_layout Plain Layout
Cite that paper which sent calculations back and forth
\end_layout

\end_inset

.
 Recently these limitations have been overcome by the emergence of General-Purpo
se Graphics Processing Units (
\emph on
GPGPUs
\emph default
), capable of running arbitrary code rather than just graphics tasks.
 Complementary programming interfaces, like Nvidia's CUDA and the less vendor-sp
ecific OpenCL, have become available for accessing the cards from mainstream
 programming languages like C, and with this integration the device specificatio
ns have been brought up to meet the precision standards required by these
 language's compilers.
 This has spurred a stream of researchers to offload their intensive computation
s to graphics cards as a more cost-effective alternative to clusters and
 conventional supercomputers.
\end_layout

\begin_layout Section
Parallel Programming
\end_layout

\begin_layout Standard
Parallel programming has existed for decades, but outside the realms of
 research and supercomputers has not been pervasive.
 The limited use of multiple 
\emph on
threads
\emph default
 of execution has become well established in areas such as interactive (includin
g graphical) programs and networked software, however this is mainly for
 stylistic reasons rather than raw performance.
 Rather than combining all of the program logic into a 
\begin_inset Quotes eld
\end_inset

main loop
\begin_inset Quotes erd
\end_inset

, different subsystems are segregated to their own functions to keep down
 code complexity.
 To prevent blocking (waiting for a function to finish) these are then further
 segregated to their own threads.
 Calls to these subsystems then become asynchronous, the next instruction
 is carried out without waiting for the call to finish.
 The reason for this being stylistic is that the main targets for such software
 are microcomputers, which until recently only had one processing core and
 thus only run one thread at a time, interleaving instructions by 
\emph on
context switching
\emph default

\begin_inset Foot
status collapsed

\begin_layout Plain Layout
This is managed by the operating system kernel, which stops and starts threads
 one at a time to simulate concurrent execution.
 To prevent threads from breaking each other, the execution environment
 (or context) for each thread is stored alongside it.
 Switching between threads thus requires switching the execution context,
 which may need to fetch values from RAM and is thus costly.
 In modern 
\emph on
preemptive multitasking
\emph default
 systems, context switching is managed by the kernel and its scheduler,
 based on thread priorities.
 The threads themselves have no control over their execution.
\end_layout

\end_inset

.
 Thus there is no direct gain in performance between multiple threads and
 one main loop, it is merely simpler and more scalable to implement multiple
 blocking services via threads.
 In fact the overhead of context switching may even incur a performance
 penalty for the threaded approach, but only if the main loop implementation
 were near-optimal (which is usually not the case).
\end_layout

\begin_layout Standard
This simple use of threads is still a useful way to express the difficulties
 in parallel programming.
 The major obstacle is that very few assumptions can be made about a parallel
 program: the interleaving of threads (or equivalent) is completely nondetermini
stic, so definitions and values may be changed by another thread at any
 point, and the granularity of this interleaving can be incredibly fine.
 A classic example to demonstrate this can be seen in 
\begin_inset Note Note
status open

\begin_layout Plain Layout
where
\end_layout

\end_inset

, which implements a hypothetical banking system.
 The top program runs on a bank server and makes payments from an account,
 whilst the bottom one runs on an ATM and allows withdrawals.
 The flaws in this are numerous, but to spot them requires a knowledge of
 this language's 
\emph on
atomic transactions
\emph default
.
 An atomic transaction is an instruction which cannot be divided into further
 instructions, hence it is atomic in the democratic sense, and it is a transacti
on since it will either be carried out or not, it cannot fail at a half-complete
d stage (otherwise it would not be atomic).
 The implementation of high-level atomic transactions (such as atomic functions)
 is an area of parallel programming research, although is difficult to realise
 due to the Two Generals problem
\begin_inset Note Note
status open

\begin_layout Plain Layout
cite
\end_layout

\end_inset

.
 Atomic transactions are thus usually very low level and reflect the architectur
e of the machine.
 The reason this becomes problematic is that if this banking system were
 compiled into machine code then the atomic transactions can not be found
 anywhere in the source code: to spot them requires knowledge of how it
 will compile!
\end_layout

\begin_layout Standard
In this example, the most damning flaws are the balance modifications.
 The reassignment of the balance by the ATM looks straighforward: the new
 balance is the old balance minus the amount withdrawn.
 However, that is certainly not what this line of code actually means.
 This becomes clearer when we see it in terms of the equivalent atomic transacti
ons, which look something like 
\begin_inset Note Note
status open

\begin_layout Plain Layout
what
\end_layout

\end_inset

.
 All values must be loaded into registers inside the CPU before they can
 be processed, but by copying the values in this way we destroy their meaning
 within our program.
 The value of 
\family typewriter
registerA
\family default
 is not the account balance, it is the account balance 
\emph on
at the time it was loaded
\emph default
.
 It is perfectly acceptable for the server to modify the balance half-way
 through an ATM withdrawal, since that's the whole point of parallel, concurrent
 execution; yet that modification will be lost when the ATM finishes the
 withdrawal, as it will set the balance to the new contents of 
\family typewriter
registerA
\family default
, which has been calculated with no knowledge of the interest payment.
 Thus a possible execution of these threads may be: The ATM loads the current
 balance in preparation for a deduction, the server sends a £200 rent payment
 to the account-holder's landlord, the ATM takes £10 from its copy of the
 balance, the server loads the account balance, takes £200 from it and stores
 the new value, the ATM stores its value then gives out £10.
 In this scenario there has been £210 taken out of the account, but only
 £10 has been deducted from the balance.
 For the computer this is perfectly valid code, however our assumptions
 about what it proves are severely at fault: the system modelled by the
 program's algebra is not isomorphic to a banking system.
\end_layout

\begin_layout Standard
This example shows how dangerous it can be to make assumptions in a parallel
 environment; how seemingly simple constructs such as 
\family typewriter
x++
\family default
 actually decompose into multiple statements, any of which can be preempted
 (and invalidated) by another thread; and why parallel programming with
 mutable, shared memory is a hard problem with hidden pitfalls.
 This is of course not the only way to program in parallel.
 Other approaches include: Software Transactional Memory, which abstracts
 memory accesses through a software library to facilitate high-level transaction
s; Actor models, which extend the Object Oriented paradigm of message passing
 to make each object a standalone Actor, which all run in parallel; Dataflow
 languages, where computations are built as networks of operations through
 which data is passed; Concurrent Functional Programming, which exploits
 pure functional programming's lack of mutability and execution order to
 distribute programs without the need for synchronisation; and Futures (or
 Promises), where values can be used in calculations even if they haven't
 been worked out yet (a kind of explicit lazy evaluation).
 The model adopted by CUDA, which will be used in this project, is threaded,
 although its usage is similar to a dataflow language.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{program}
\end_layout

\begin_layout Plain Layout


\backslash
mbox{A simple bank server which provides monthly services:}
\end_layout

\begin_layout Plain Layout


\backslash
WHILE |keepRunning| 
\backslash
DO
\end_layout

\begin_layout Plain Layout

 
\backslash
COMMENT{See if this month's payments have been made}
\end_layout

\begin_layout Plain Layout

 
\backslash
IF |lastPayments| < |thisMonth|
\end_layout

\begin_layout Plain Layout

  |balance| := |balance| 
\backslash
times (1+|rate|) 
\backslash
rcomment{Add on interest}
\end_layout

\begin_layout Plain Layout

  
\backslash
FOR |order | in | standingOrders| 
\backslash
DO
\end_layout

\begin_layout Plain Layout

   |pay|(|order|.|recipient|,|order|.|amount|)
\end_layout

\begin_layout Plain Layout

   |balance| := |balance|-|order|.|amount|
\end_layout

\begin_layout Plain Layout

  |lastPayments|++
\end_layout

\begin_layout Plain Layout


\backslash
end{program}
\end_layout

\begin_layout Plain Layout


\backslash
begin{program}
\end_layout

\begin_layout Plain Layout


\backslash
mbox{A complementary program for an ATM:}
\end_layout

\begin_layout Plain Layout


\backslash
WHILE |keepRunning| 
\backslash
DO
\end_layout

\begin_layout Plain Layout

 
\backslash
IF |pin| = |accountPin| 
\backslash
land |action| = |withdraw|
\end_layout

\begin_layout Plain Layout

  |balance| := |balance| - |amount|
\end_layout

\begin_layout Plain Layout

  |give|(|amount|)
\end_layout

\begin_layout Plain Layout


\backslash
end{program}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
A subtley flawed banking system, with simple algorithms for the server and
 ATM 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{program}
\end_layout

\begin_layout Plain Layout


\backslash
mbox{Atomic instructions in pseudo-x86 machine language}
\end_layout

\begin_layout Plain Layout

|loadValue|(|balance|,|registerA|)
\end_layout

\begin_layout Plain Layout

|loadValue|(|amount|,|registerB|)
\end_layout

\begin_layout Plain Layout

|subtract|(|registerA|,|registerB|)
\end_layout

\begin_layout Plain Layout

|storeValue|(|balance|,|registerA|)
\end_layout

\begin_layout Plain Layout


\backslash
end{program}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
A deconstruction of the ATM's balance update line.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
The CUDA Model
\end_layout

\begin_layout Standard
Since this project focuses specifically on the use of Nvidia's CUDA (Compute
 Unified Device Architecture
\begin_inset Note Note
status open

\begin_layout Plain Layout
is that right?
\end_layout

\end_inset

) interface, a more specific description of its operation and use from C
 is given here.
\end_layout

\begin_layout Subsection
Memory Hierarchy
\end_layout

\begin_layout Standard
CUDA is an interface to Nvidia graphics cards, a topological overview of
 which can be seen in
\begin_inset Note Note
status open

\begin_layout Plain Layout
add figure
\end_layout

\end_inset

.
 The components are structured in a hierarchy based around memory access:
 The graphics card's RAM (
\emph on
global memory
\emph default
) is the root of the tree, globally accessible but relatively slow 
\begin_inset Note Note
status open

\begin_layout Plain Layout
quote a number
\end_layout

\end_inset

.
 Below this is the 
\emph on
shared memory
\emph default
 of the multiprocessors, connected to global memory through a cache.
 This memory is accessible to every core in its multiprocessor, and this
 ultimately determines the coarse-grain structure of the algorithms used.
 Finally each core in a multiprocessor (there are 8) has its own 
\emph on
registers
\emph default
, directly read from and written to by the Arithmetic Logic Units (the mathemati
cal circuitry) of each core.
 Although important to keep in mind, registers will not be explicitly used
 in the project as the access times are not too dissimilar to shared memory
 access times
\begin_inset Note Note
status open

\begin_layout Plain Layout
cite
\end_layout

\end_inset

.
 However, the difference in lookup time between shared and global memory
 can be as much as 15,000%
\begin_inset Note Note
status open

\begin_layout Plain Layout
cite
\end_layout

\end_inset

, so this should be the main performance concern at the algorithm design
 stage.
 
\emph on
Host memory
\emph default
, ie.
 the motherboard's RAM, is so slow that it should not be considered an option
 for storage during computations.
 In fact, the usual bottleneck for CUDA programs is the bandwidth of copying
 data between CPU and GPU.
 Thus for any performance gains to be made, all computation should take
 place solely on the GPU, with the results transferred back at the end.
\end_layout

\begin_layout Subsection
Code Hierarchy
\end_layout

\begin_layout Standard
CUDA separates CPU code from GPU code, and hides the graphics card from
 the host.
 Although it is not directly accessible to C programs in the way that RAM
 and registers are accessible, CUDA does provide functions to handle the
 device.
 For example a call to 
\family typewriter
cudaMemcpy(void* destination, void* source, int size, cudaMemcpyHostToDevice)
\family default
 will transfer 
\family typewriter
size
\family default
 dereferenced values from the pointer 
\family typewriter
source
\family default
 to the pointer 
\family typewriter
destination
\family default
.
 In this case the flag 
\family typewriter
cudaMemcpyHostToDevice
\family default
 is passed, indicating that 
\family typewriter
source
\family default
 points to a location in the host's memory whilst 
\family typewriter
destination
\family default
 points to a location in the GPU's global memory.
 The set-up and tear-down for a calculation is completely accomplished through
 the use of these functions, and is necessarily performed from host-side
 in (mostly) serial C.
 Ideally no other work should be done by the host, but this may be difficult
 in practice.
\end_layout

\begin_layout Standard
Code destined for the GPU is written in regular C with a few deviations.
 Firstly there can be no recursion, which unfortunately makes CUDA an unsuitable
 target for many elegant recursive algorithms.
 Secondly there can be no function pointers; this doesn't present too much
 of a problem for small projects with known inputs.
 More specific idiosyncracies will be covered where appropriate.
\end_layout

\begin_layout Standard
To facilitate the massive parallelism, CUDA code is designated as one of
 three distinct kinds of function: 
\emph on
host functions
\emph default
 (regular C functions running on the host), 
\emph on
device functions
\emph default
 (which run on the GPU and can only be called from there) and 
\emph on
kernel functions
\emph default
 (which run on the GPU but are invoked by the host).
 Host functions are specified in the usual way, whilst device and kernel
 functions have their signatures augmented with the keywords 
\family typewriter
__device__
\family default
 and 
\family typewriter
__global__
\family default
 respectively.
 In listing 
\begin_inset Note Note
status open

\begin_layout Plain Layout
what
\end_layout

\end_inset

 we see a simple CUDA program with prototypes for a host function 
\family typewriter
main
\family default
 (our entry point), a kernel function 
\family typewriter
foo
\family default
 and two device functions 
\family typewriter
bar
\family default
 and 
\family typewriter
baz
\family default
.
 Figure 
\begin_inset Note Note
status open

\begin_layout Plain Layout

\family typewriter
what
\end_layout

\end_inset

 shows which functions can call each other.
 Note that since recursion is not allowed for device or kernel functions,
 any dependency between 
\family typewriter
bar
\family default
 and 
\family typewriter
baz
\family default
 must be mutually exclusive.
 To initiate computation on the GPU requires calling a kernel function from
 some host code, which uses its own syntax of the form 
\family typewriter
function_name<<<dimGrid, dimBlock>>>(arguments);
\family default
 where 
\family typewriter
arguments
\family default
 is a normal comma-separated list of arguments for the function, whilst
 
\family typewriter
dimGrid
\family default
 and 
\family typewriter
dimBlock
\family default
 will be explained in the next section.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
lstset{language=C}
\end_layout

\begin_layout Plain Layout


\backslash
begin{lstlisting}[frame=single]
\end_layout

\begin_layout Plain Layout

__device__ int bar(int arg1);
\end_layout

\begin_layout Plain Layout

__device__ int baz(int arg1);
\end_layout

\begin_layout Plain Layout

__global__ void foo(int arg1);
\end_layout

\begin_layout Plain Layout

int main();
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Definition of four CUDA functions: main is a host function, foo is a kernel
 function whilst bar and baz are device functions.
 Return and argument types are arbitrary, except that kernel functions must
 return void.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename device_kernel_host_reachability.svg
	height 25pheight%

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Function hierarchy: host code can call kernel functions, kernel functions
 can call device functions and device functions can call each other (non-recursi
vely)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Thread Hierarchy
\end_layout

\begin_layout Standard
CUDA has several layers of organisation for threads.
 Each 
\emph on
thread
\emph default
 is part of a 
\emph on
thread block
\emph default
, and each thread block is part of a 
\emph on
grid
\emph default
.
 The grid and its blocks each have 
\emph on
dimensions
\emph default
, similar to an array; purportedly for convienience reasons, but actually
 a necessity due to CUDA's in-built limitations.
 The idea of block dimensions is to give each thread a location is a space.
 For a one-dimensional block, each thread can get its location via the name
 
\family typewriter
threadIdx.x
\family default
.
 In a two-dimensional block there is a non-unique 
\family typewriter
threadIdx.x
\family default
 specifying the 'column' and a non-unique 
\family typewriter
threadIdx.y
\family default
 specifying the 'row'.
 These can be combined to give a unique ID for the thread if the size of
 the block is known, which can be accessed as 
\family typewriter
blockDim.x
\family default
 and 
\family typewriter
blockDim.y
\family default
.
 Thus 
\begin_inset Formula $\text{{threadIdx.x}+\left(\text{{blockDim.x}\times}\text{{threadIdx.y}}\right)}$
\end_inset

is unique to each thread
\begin_inset Note Note
status open

\begin_layout Plain Layout
right way around?
\end_layout

\end_inset

.
 Similarly in three dimensions there is a 
\family typewriter
threadIdx.z
\family default
 and a 
\family typewriter
blockDim.z
\family default
 which can give a unique ID via
\family typewriter
 
\begin_inset Formula $\text{{threadIdx.x}+\left(\text{{blockDim.x}\times}\text{{threadIdx.y}}\right)+\left(\text{\left(\text{{blockDim.x}\times}\text{{blockDim.y}}\right)\times}\text{{threadIdx.z}}\right)}$
\end_inset


\family default
.
 Note that these values are always defined, but are set to 1 if not used
 (for example in a one dimensional block, 
\begin_inset Formula $\text{{threadIdx.y}}=\text{{threadIdx.z}=\text{{blockDim.y}=\text{{blockDim.z}=}}}1$
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
is there a fourth?
\end_layout

\end_inset

 The same is true for a grid of blocks, except it can only be one or two
 dimensional.
 The equivalent bindings are 
\family typewriter
gridDim.x
\family default
, 
\family typewriter
gridDim.y
\family default
, 
\family typewriter
blockIdx.x
\family default
 and 
\family typewriter
blockIdx.y
\family default
.
 There are no grid index (Idx) bindings since there is only ever one grid.
\end_layout

\begin_layout Standard
To specify these dimensions, the extended syntax for the kernel function
 invocation is used.
 The first argument gives the grid dimensions and the second gives the block
 dimensions.
 These have the type dim3 (part of the CUDA libraries) and unspecified dimension
s default to 1.
 Thus a single integer specifies a one-dimensional structure, whilst a call
 such as dim3(x,y) specifies a two dimensional 
\begin_inset Formula $x\times y$
\end_inset

 structure and dim3(x,y,z) gives a three dimensional 
\begin_inset Formula $x\times y\times z$
\end_inset

 structure.
 These can be used within the triple arrows, but remember that grids (the
 first argument) can be a maximum of two dimensional.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
lstset{language=C}
\end_layout

\begin_layout Plain Layout


\backslash
begin{lstlisting}[frame=single]
\end_layout

\begin_layout Plain Layout

__global__ void foo(float* A, float* B, float* C) {
\end_layout

\begin_layout Plain Layout

  C[threadIdx.x] = A[threadIdx.x] + B[threadIdx.x];
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Simple CUDA kernel function to add two vectors in parallel.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The unique IDs defined in this way are very useful for distributing a workload,
 as they allow unique indexing of an array.
 For example, consider the simple kernel function in
\begin_inset Note Note
status open

\begin_layout Plain Layout
what
\end_layout

\end_inset

 for summing two vectors.
 It calculates the vector 
\begin_inset Formula $C$
\end_inset

 from the relationship 
\begin_inset Formula $A_{i}+B_{i}=C_{i}$
\end_inset

, using the mapping 
\begin_inset Formula $i\equiv\text{{threadIdx.x}}$
\end_inset

 to ensure that every element is calculated exactly once.
 When invoked by a host function with a call such as 
\family typewriter
foo<<<1,N>>>(A, B, C);
\family default
, each element of 
\begin_inset Formula $C$
\end_inset

 will be calculated in its own thread (with N threads in total, requiring
 at least N elements).
 This usage of CUDA is typical, and allows a stream processing/dataflow
 style within the threaded model.
 Moreover, allowing threads to obtain unique indices prevents conflicts,
 since there is a simple way to ensure threads are accessing different areas
 of memory and thus not overwriting each other.
\end_layout

\begin_layout Section
Smoothed-Particle Hydrodynamics
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{program}
\end_layout

\begin_layout Plain Layout


\backslash
mbox{Serial SPH algorithm}
\end_layout

\begin_layout Plain Layout


\backslash
FOR |particle1 | in | particles| 
\backslash
DO
\end_layout

\begin_layout Plain Layout

 
\backslash
FOR |particle2 | in | particles| 
\backslash
DO
\end_layout

\begin_layout Plain Layout

  
\backslash
IF |particle1| 
\backslash
neq |particle2|
\end_layout

\begin_layout Plain Layout

   |interact|(|particle1|, |particle2|)
\end_layout

\begin_layout Plain Layout

  
\backslash
FI
\end_layout

\begin_layout Plain Layout

 
\backslash
OD
\end_layout

\begin_layout Plain Layout


\backslash
OD
\end_layout

\begin_layout Plain Layout


\backslash
FOR |particle | in | particles| 
\backslash
DO
\end_layout

\begin_layout Plain Layout

 |update|(|particle|)
\end_layout

\begin_layout Plain Layout


\backslash
OD
\end_layout

\begin_layout Plain Layout


\backslash
end{program}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:Naive-serial-implementation"

\end_inset

Naive serial implementation of Smoothed-Particle Hydrodynamics
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{program}
\end_layout

\begin_layout Plain Layout


\backslash
mbox{Parallel SPH algorithm for NxN threads with sequential IDs}
\end_layout

\begin_layout Plain Layout

|i| := |ID| 
\backslash
% N
\end_layout

\begin_layout Plain Layout

|j| := (|ID| - |i|) / N
\end_layout

\begin_layout Plain Layout

|interact|(|particles|[|i|], |particles|[|j|])
\end_layout

\begin_layout Plain Layout

|sync|()
\end_layout

\begin_layout Plain Layout


\backslash
IF |i| = 0
\end_layout

\begin_layout Plain Layout

 |update|(|particles|[|j|])
\end_layout

\begin_layout Plain Layout


\backslash
FI
\end_layout

\begin_layout Plain Layout


\backslash
end{program}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:Naive-parallel-implementation"

\end_inset

Naive parallel implementation of Smoothed-Particle Hydrodynamics
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The Physical model being simulated in this project is a 
\emph on
Smoothed-Particle Hydrodynamics
\emph default
 (SPH) system.
 In this model, the behaviour of a large number of atoms and molecules in
 a fluid is approximated with a much smaller number of 
\emph on
particles
\emph default
 with a much larger spatial extent.
 These particles carry various bulk properties in their representation of
 the fluid, which are mutated over time.
 For this computational problem will only concern ourselves with the spatial
 properties: the coordinates and the characteristic size, known as the 
\emph on
smoothing length
\emph default
, of each particle.
 In a perfect SPH model, the interactions between every particle with every
 other particle are calculated, the properties are then updated based on
 the results, then the process is repeated.
 This models the time-evolution of the system by approximating a time integral
 with steps.
\end_layout

\begin_layout Standard
To process these interactions requires 
\begin_inset Formula $O\left(N_{particles}^{2}\right)$
\end_inset

 operations, whilst to update the properties requires 
\begin_inset Formula $O\left(N_{particles}\right)$
\end_inset

.
 For a serial algorithm, such as the loop shown in algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Naive-serial-implementation"

\end_inset

, these translate directly into time.
 However, there is no dependency between the calculations inside a step,
 making this an embarassingly parallel problem.
 Thus a parallel algorithm could work them all out at once using 
\begin_inset Formula $N_{particles}^{2}$
\end_inset

 processors and do the same job in constant time, as seen in algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Naive-parallel-implementation"

\end_inset

.
\end_layout

\begin_layout Standard
This is still incredibly wasteful, and we can trim down the calculations
 to perform considerably by exploiting the nature of the model.
 In SPH each particle has a 
\begin_inset Quotes eld
\end_inset

shape
\begin_inset Quotes erd
\end_inset

 defined by a 
\emph on
kernel function
\emph default
 (not to be confused with the concept of a kernel function in CUDA!), which
 governs the amplitude of particle interactions.
 Typical functions are Gaussians and cubic splines, but can be anything
 which falls off over distance.
 The rate of fall off is governed by the smoothing length, but in the case
 of something like a Gaussian it never quite reaches zero.
 Hence every particle 
\emph on
should
\emph default
 interact with all of the others, but the individual contributions become
 negligible after a long enough distance.
 A safe approximation, therefore, is to limit the interactions to those
 particles falling within this distance of each other, the 
\emph on
nearest neighbours
\emph default
.
 This brings down the number of computations to perform, however the number
 of nearest neighbours depends on the density and is thus still a linear
 function of 
\begin_inset Formula $N_{particles}$
\end_inset

, keeping our order of magnitude at 
\begin_inset Formula $O\left(N_{particles}^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Another insightful fact about SPH is that we can derive more accurate results
 if the smoothing length becomes a function of the local density, with the
 length contracting for more densely packed spaces.
 This allows us to more efficiently concentrate out simulation efforts,
 as we can waste as few particles as possible in boring, sparsely populated
 regions, and in return we get a higher resolution for the areas containing
 more interactions.
 This refinement, as well as bringing efficiency in particle rationing,
 also brings down the number of nearest neighbours.
 In fact, by making the smoothing length decrease with 
\begin_inset Formula $N_{particles}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
N_{nearestneighbours}\left(N_{particles}\right) & \propto & \underset{\text{{density}}}{\underbrace{\frac{N_{particles}}{V_{space}}}}\times\underset{\text{(normalised) volume of interaction}}{\underbrace{\frac{L_{smoothing}\left(N_{particles}\right)^{3}}{V_{space}}}}\\
 & \propto & N_{particles}\times L_{smoothing}\left(N_{particles}\right)^{3}\\
 & \propto & N_{particles}\times V_{nearestneighbours}\left(N_{particles}\right)\\
 & \propto & N_{particles}\times\frac{1}{N_{particles}}\\
 & \propto & 1\end{eqnarray*}

\end_inset

Hence the number of neighbours is on the order of constant, making each
 particle's interactions 
\begin_inset Formula $O\left(1\right)$
\end_inset

 and thus the whole system's interactions 
\begin_inset Formula $O\left(N_{particles}\right)$
\end_inset

.
 This saves us a factor of 
\begin_inset Formula $N_{particles}$
\end_inset

 calculations by restricting ourselves to the nearest neighbours, which
 are the only non-negligible contributions.
\end_layout

\begin_layout Standard
Whilst this speedup seems simple, in fact it is deceptive.
 In order to work out which particles are nearest neighbours, we need to
 compare their positions.
 To find all of the nearest neighbours of a particle is thus an 
\begin_inset Formula $O\left(N_{particles}\right)$
\end_inset

 operation, and to find all of the nearest neighbours for the system is
 
\begin_inset Formula $O\left(N_{particles}^{2}\right)$
\end_inset

 and we are back to where we started.
 However there are algorithms to help us find these neighbours with fewer
 comparisons, which this project implements.
\end_layout

\begin_layout Subsection
The Gridding Algorithm
\end_layout

\begin_layout Standard
It would seem useful to keep track of each particle's nearest neighbours
 once they've been identified, but this would not help us since the particles
 move between time steps, which changes their neighbours from step to step.
 One method of tracking particles that isn't invalidated at each step is
 to construct a grid and track each particle's grid cell.
 By choosing a grid size equal to the largest interaction radius in the
 system, we can ensure that a particle and its nearest neighbours are always
 either in the same grid cell or at most one cell apart.
 This narrows down our search for nearest neighbours considerably, since
 we can concentrate our efforts on those neighbouring cells.
 It is important to note that the efficiency of this method is reduced when
 the difference between the largest interaction radius and the average interacti
on radius becomes large (this is due to overcrowded cells, where most of
 the particles in neighbouring cells are not nearest neighbours and are
 thus wasted comparisons), in other words when the density is highly non-uniform.
 It is important at this point to note the difference between a grid of
 cell containing particles, decribed in this paragraph, and CUDA's grid
 of blocks containing threads, described previously.
 They have no relation in this implementation
\begin_inset Foot
status open

\begin_layout Plain Layout
An attempt was made to model the SPH grid directly with a CUDA grid, but
 the two-dimensional limitation of CUDA grids prevented this.
\end_layout

\end_inset

.
\end_layout

\begin_layout Section
Implementation
\end_layout

\begin_layout Standard
There are many ways to implement the gridding algorithm for Smoothed-Particle
 Hydrodynamics.
 It was important to keep the solutions general, such that any datastructures
 and algorithms could be used to model the particles and interactions, so
 no assumptions could be made about either except that the particles contain
 three-dimensional Cartesian coordinates and a size.
\end_layout

\begin_layout Standard
Four different approaches have been taken for this project, which will be
 referred to as the 
\emph on
linked list
\emph default
, 
\emph on
sentinel
\emph default
, 
\emph on
pair-array
\emph default
 and 
\emph on
padded
\emph default
 implementations.
 Other possibilities were investigated but ultimately abandoned, the reasons
 for this are explored in Appendix 
\begin_inset Note Note
status open

\begin_layout Plain Layout
what
\end_layout

\end_inset

.
\end_layout

\begin_layout Section
\start_of_appendix
Unworkable Solutions
\end_layout

\begin_layout Standard
There were several approaches to the gridding algorithm considered but never
 implemented.
 These range from minor details to overarching conceptual shifts.
 A few are explored here to illustrate what is and is not a reasonable course
 to take when modelling parallel software in C and CUDA.
\end_layout

\begin_layout Subsection
Pure Pointer Arithmetic
\end_layout

\begin_layout Standard
In C the physical location of data is reified into data itself through the
 concept of 
\emph on
pointers
\emph default
, invoked by the address-of operator &.
 Pointers are pervasive in the C approach to programming, as they are the
 only way to achieve certain tasks (short of Godel numbering).
 A pointer is a positive (unsigned) integer which is an address in RAM which
 holds a certain piece of data.
 Pointers also have a 
\emph on
type
\emph default
 (in the loose, C sense of the term) which exists solely to provide a length.
 Upon 
\emph on
dereference
\emph default
, written foo[index] where foo is a pointer and index an integer offset,
 an amount of data is fetched from RAM starting at the location 
\begin_inset Formula $\text{{foo}}+\left(\text{{sizeof}}\left(\text{{foo's}type}\right)\times\text{{index}}\right)$
\end_inset

 and of length sizeof(foo's type).
 A shorthand for foo[0] is *foo.
\end_layout

\begin_layout Standard
The direct manipulation of memory in this way allows an instantiation of
 the array datastructure, a random-access, fixed-size construct for storing
 a subset of the array's type.
 The exploitation of these abstracted structures and their underlying pointer
 implementation seemed immediately applicable to the problem of implementing
 the grid: namely, how to specify the contents of a cell with as little
 overhead as possible.
 For zero- and one-dimensional grids, the implementation using an array
 abstraction was obvious, along the lines of [cell 0, cell 1, cell 2, ...,
 cell N], but the problem of unequal numbers of particles in each cell would
 cause either an extra dimension to be used, which although conceptually
 close would in fact cause non-contiguous storage in memory, or the loss
 of constant-time lookups (by reading through the array).
 The use of pointer arithmetic on 
\begin_inset Quotes eld
\end_inset

arrays
\begin_inset Quotes erd
\end_inset

 (pointers) can solve this, by using a small collection of integers to refer
 to the various cell locations, and by incrementing and decrementing the
 pointers to expand and shrink the borders of the cells.
 In effect, the physical space of the grid can be implemented directly inside
 the RAM, such that the only operations necessary for the entire gridding
 algorithm are pointer increment and decrement.
\end_layout

\begin_layout Standard
Unfortunately the main advantage of this method is lost when higher dimensions
 are modelled, since contemporary RAM is one-dimensional which is insufficient
 to represent cell boundaries of two or more dimensions.
 Rather than achieving everything with fast ++ and -- operators on the grid
 metadata, discontinuities would have to be taken into account when mutating
 cell contents in a higher dimension, and even worse it would be necessary
 to touch the particle data as it is shuffled from one cell (array) to another
 (since the arrays cannot shuffle around the particles in more than the
 x dimension).
\end_layout

\begin_layout Standard
This approach was therefore abandoned, despite the obvious simplicity and
 performance gains it would bring.
 However, it is certainly a worthy option to consider for one-dimensional
 models and machines with higher-dimensional memory.
\end_layout

\end_body
\end_document
